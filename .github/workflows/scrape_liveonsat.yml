# اسم الورك فلو
name: Scrape, Filter, and Translate Matches

on:
  schedule:
    - cron: "*/15 * * * *"  # كل 15 دقيقة (لتجنب الضغط على الخدمة)
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-filter:
    runs-on: ubuntu-latest

    steps:
      # 1. نسخ كود المشروع
      - name: Checkout repo
        uses: actions/checkout@v4

      # 2. تنصيب بايثون
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # 3. تثبيت كل المكتبات المطلوبة (مع إضافة googletrans)
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright beautifulsoup4 googletrans==4.0.0-rc1
          playwright install --with-deps chromium

      # 4. تشغيل سكربت السحب الأول
      - name: Run Scraper (scrape_liveonsat_only.py)
        run: python scripts/scrape_liveonsat_only.py

      # 5. تشغيل سكربت الفلترة والترجمة الثاني
      - name: Run Filter and Translator (filter_json.py)
        run: python scripts/filter_json.py

      # 6. حفظ الملفين الناتجين في الريبو
      - name: Commit & push if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          # رسالة كوميت واضحة
          commit_message: "📊 Chore: Update scraped and filtered match data"
          # تحديد الملفين للحفظ
          file_pattern: "matches/liveonsat_raw.json matches/filtered_matches.json"
