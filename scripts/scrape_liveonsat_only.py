# scripts/scrape_liveonsat_only.py
import re
import json
import time
from pathlib import Path

import requests
from bs4 import BeautifulSoup

REPO_ROOT = Path(__file__).resolve().parents[1]
OUT_DIR = REPO_ROOT / "matches"
OUT_DIR.mkdir(parents=True, exist_ok=True)
OUT_PATH = OUT_DIR / "liveonsat_raw.json"

URL = "https://liveonsat.com/2day.php"

HEADERS = {
    "user-agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/127.0.0.0 Safari/537.36"
    ),
    "accept-language": "en-US,en;q=0.9,ar;q=0.8",
    "cache-control": "no-cache",
    "pragma": "no-cache",
}

def fetch_html(url: str, retries: int = 3, timeout: int = 45) -> str:
    last_err = None
    for i in range(retries):
        try:
            r = requests.get(url, headers=HEADERS, timeout=timeout)
            r.raise_for_status()
            return r.text
        except Exception as e:
            last_err = e
            time.sleep(1 + i * 2)
    raise last_err

def text_clean(s: str) -> str:
    if not s:
        return ""
    # Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³Ù… ÙƒÙ…Ø§ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙÙ‚Ø· Ù†Ø´ÙŠÙ„ Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ÙˆØ§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ù…Ø±ÙÙ‚Ø©
    return (
        s.replace("ğŸ“º", "")
         .replace("[$]", "")
         .strip()
    )

def guess_match_title(block: BeautifulSoup) -> str:
    """
    Ù†Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ù†ÙØ³ Ø§Ù„ÙƒØªÙ„Ø© Ø¹Ù† Ø³Ø·Ø± ÙÙŠÙ‡ ' v ' (Ù…Ø«Ù„: Brentford v Chelsea).
    Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø±Ø¬Ø¹ Ù†ØµÙ‹Ø§ ÙØ§Ø¶ÙŠÙ‹Ø§.
    """
    txt = " ".join(block.get_text(" ", strip=True).split())
    # Ø§Ù„ØªØ±ØªÙŠØ¨: league line Ø«Ù… title Ø«Ù… ST Ø«Ù… Ø§Ù„Ù‚Ù†ÙˆØ§Øª. Ù†Ù„ØªÙ‚Ø· Ø£Ù‚Ø±Ø¨ Ø¹Ù†ÙˆØ§Ù† Ù‚Ø¨Ù„ ST.
    # Ù†Ø¬Ø±Ù‘Ø¨ Ø£ÙˆÙ„Ø§Ù‹ Ø§Ù„ØªÙ‚Ø§Ø· ÙƒÙ„ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:
    cands = re.findall(r"([^\n\r]+?\s+v\s+[^\n\r]+?)", txt, flags=re.IGNORECASE)
    if cands:
        # Ù†Ø§Ø®Ø° Ø£Ù‚ØµØ±/Ø£ÙˆØ¶Ø­ ÙˆØ§Ø­Ø¯Ø© (ØºØ§Ù„Ø¨Ø§Ù‹ ØªÙƒÙˆÙ† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©)
        cands = sorted(set(cands), key=len)
        return cands[0]
    return ""

def split_home_away(title: str):
    m = re.search(r"(.+?)\s+v\s+(.+)", title, flags=re.IGNORECASE)
    if m:
        return m.group(1).strip(), m.group(2).strip()
    return None, None

def parse_liveonsat(html: str):
    """
    ÙŠØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¨Ø§Ø±ÙŠØ§Øª:
    [
      {
        "match_title": "Brentford v Chelsea",
        "home": "Brentford",
        "away": "Chelsea",
        "time_st": "22:00",
        "channels": ["beIN Sports MENA 1 HD", "Sky Sports Premier League HD", ...]
      },
      ...
    ]
    """
    soup = BeautifulSoup(html, "html.parser")

    # Ø§Ù„ÙÙƒØ±Ø©: ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù‚Ù†ÙˆØ§Øª Ù„Ù‡Ø§ div.fLeft_time_live (ÙŠØ­Ù…Ù„ ST: HH:MM) ÙˆÙ…Ø¹Ù‡ div.fLeft_live ÙÙŠÙ‡ Ø¹Ø¯Ø© Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ù†ÙˆØ§Øª.
    # Ù‡Ù†Ù…Ø´ÙŠ Ø¹Ù„Ù‰ ÙƒÙ„ div.fLeft_time_live ÙˆÙ†Ø±Ø¨Ø·Ù‡ Ø¨Ø£Ù‚Ø±Ø¨ ÙƒØªÙ„Ø© Ø¹Ù„ÙŠØ§ ØªØ­ØªÙˆÙŠÙ‡ (Ø­ØªÙ‰ Ù†Ø³ØªØ®Ø±Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø© ÙˆØ§Ù„Ù‚Ù†ÙˆØ§Øª).
    results_map = {}  # key: (title, time) -> list channels

    time_divs = soup.select("div.fLeft_time_live")
    for tdiv in time_divs:
        raw_time = tdiv.get_text(strip=True) or ""
        mt = re.search(r"ST:\s*(\d{1,2}:\d{2})", raw_time)
        if not mt:
            continue
        time_st = mt.group(1)

        # Ø§Ø¨Ø­Ø« Ø¹Ù† div.fLeft_live Ø¶Ù…Ù† Ù†ÙØ³ Ø§Ù„ÙƒØªÙ„Ø©
        live_div = None
        parent = tdiv.parent
        for _ in range(6):
            if parent is None:
                break
            live_div = parent.select_one("div.fLeft_live")
            if live_div:
                break
            parent = parent.parent

        if not live_div:
            live_div = tdiv.find_next("div", class_="fLeft_live")
        if not live_div:
            continue

        # Ø§Ù„Ù‚Ù†ÙˆØ§Øª: ÙƒÙ„ <a> Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        channels = []
        for a in live_div.select("table a"):
            name = text_clean(a.get_text(" ", strip=True))
            if name:
                channels.append(name)
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        seen = set()
        uniq_channels = []
        for c in channels:
            if c not in seen:
                seen.add(c)
                uniq_channels.append(c)

        # Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©: Ù†Ø­Ø§ÙˆÙ„ Ù…Ù† Ù†ÙØ³ ÙƒØªÙ„Ø© parent Ø§Ù„Ø¹Ù„ÙŠØ§
        block = parent if parent else tdiv
        match_title = guess_match_title(block)
        if not match_title:
            # fallback: Ø´ÙˆÙ Ù‚Ø¨Ù„ div Ø§Ù„ÙˆÙ‚Øª Ø¨Ù‚Ù„ÙŠÙ„
            prev_txt = " ".join((tdiv.find_previous(string=True) or "").split())
            pm = re.search(r"(.+?)\s+v\s+(.+)", prev_txt, flags=re.IGNORECASE)
            if pm:
                match_title = pm.group(0).strip()

        key = (match_title, time_st)
        if key in results_map:
            # Ø¯Ù…Ø¬ Ù‚Ù†ÙˆØ§Øª Ù„Ùˆ ØªÙƒØ±Ø±Øª Ù†ÙØ³ Ø§Ù„Ø®Ø§Ù†Ø©
            existing = results_map[key]
            for c in uniq_channels:
                if c not in existing:
                    existing.append(c)
        else:
            results_map[key] = uniq_channels

    # ØµÙŠØ§ØºØ© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    matches = []
    for (title, time_st), chans in results_map.items():
        home, away = split_home_away(title) if title else (None, None)
        matches.append({
            "match_title": title or None,
            "home": home,
            "away": away,
            "time_st": time_st,
            "channels": chans
        })

    # ØªØ±ØªÙŠØ¨ Ø§Ø®ØªÙŠØ§Ø±ÙŠ: Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø«Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    def sort_key(m):
        return (m["time_st"] or "99:99", m["match_title"] or "")
    matches.sort(key=sort_key)
    return matches

def main():
    print("[LiveOnSat] GET", URL)
    html = fetch_html(URL, retries=3, timeout=45)
    print("[LiveOnSat] parsing ...")
    matches = parse_liveonsat(html)
    out = {
        "source": URL,
        "count": len(matches),
        "matches": matches
    }
    OUT_PATH.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"[write] {OUT_PATH}  (matches={len(matches)})")

if __name__ == "__main__":
    main()
