# scripts/filter_json.py
import json
import re
import unicodedata
from pathlib import Path
import requests
import os

# Ù…Ø­Ø±ÙƒØ§Øª ØªØ±Ø¬Ù…Ø©/Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø©
try:
    from deep_translator import GoogleTranslator
except Exception:
    GoogleTranslator = None

try:
    from rapidfuzz import fuzz
except Exception:
    fuzz = None

# === Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ===
REPO_ROOT = Path(__file__).resolve().parents[1]
MATCHES_DIR = REPO_ROOT / "matches"
INPUT_PATH = MATCHES_DIR / "liveonsat_raw.json"   # Ù†ÙƒÙ…Ù„ Ù…Ù†Ù‡ Ø§Ù„Ù‚Ù†ÙˆØ§Øª
OUTPUT_PATH = MATCHES_DIR / "filtered_matches.json"
YALLASHOOT_URL = "https://raw.githubusercontent.com/a7shk1/yallashoot/refs/heads/main/matches/today.json"

FUZZY_THRESHOLD = int(os.getenv("LIVO_FUZZY_THRESHOLD", "84"))

# === Ø£Ø¯ÙˆØ§Øª Ø¹Ø§Ù…Ø© ===
AR_LETTERS_RE = re.compile(r'[\u0600-\u06FF]')
EMOJI_MISC_RE = re.compile(r'[\u2600-\u27BF\U0001F300-\U0001FAFF]+')
BEIN_RE = re.compile(r'bein\s*sports?', re.I)

GARBAGE_TOKENS_RE = re.compile(
    r"""
    (\(\s*\$?\/?geo\/?R\s*\))|        # ($/geo/R) Ø£Ùˆ (geo/R)
    (\(\s*geo\/?R\s*\))|
    (\(\s*\$\/?geo\s*\))|
    (\$\/?geo\/?R)|
    (ğŸ“º)|(\[online\])|(\[app\])       # Ø±Ù…ÙˆØ² Ù…ØªÙØ±Ù‚Ø©
    """, re.I | re.X
)

def strip_accents(s: str) -> str:
    return ''.join(c for c in unicodedata.normalize("NFKD", s) if not unicodedata.combining(c))

def normalize_text(text: str) -> str:
    if not text:
        return ""
    text = str(text)
    text = EMOJI_MISC_RE.sub("", text)
    text = re.sub(r"\(.*?\)", "", text)
    text = strip_accents(text)
    text = text.lower()
    text = text.replace("&", "and")
    text = re.sub(r"\b(fc|sc|cf)\b", "", text)
    text = text.replace(" ", "").replace("-", "").replace("_", "")
    text = text.replace("Ø§Ù„", "")
    text = re.sub(r"[^a-z0-9\u0600-\u06FF]", "", text)
    return text.strip()

def unique_preserving(seq):
    seen, out = set(), []
    for x in seq:
        k = str(x).lower().strip()
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out

def clean_channel_display(name: str) -> str:
    if not name:
        return ""
    s = str(name)
    s = EMOJI_MISC_RE.sub("", s)
    s = GARBAGE_TOKENS_RE.sub("", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def is_bein_channel(name: str) -> bool:
    return bool(BEIN_RE.search(name or ""))

# === Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© (ÙÙ„ØªØ±Ø© ØµØ§Ø±Ù…Ø©) ===
SUPPORTED_CHANNELS = [
    "MATCH! Futbol 1", "MATCH! Futbol 2", "MATCH! Futbol 3",
    "Football HD",
    "Sport TV1 Portugal HD", "Sport TV2 Portugal HD",
    "ESPN 1 Brazil", "ESPN 2 Brazil", "ESPN 3 Brazil", "ESPN 4 Brazil", "ESPN 5 Brazil", "ESPN 6 Brazil", "ESPN 7 Brazil",
    "DAZN 1 Portugal HD", "DAZN 2 Portugal HD", "DAZN 3 Portugal HD", "DAZN 4 Portugal HD", "DAZN 5 Portugal HD", "DAZN 6 Portugal HD",
    "MATCH! Premier HD", "Sky Sports Main Event HD", "Sky Sport Premier League HD", "IRIB Varzesh HD",
    "Persiana Sport HD", "MBC Action HD", "TNT Sports 1 HD", "TNT Sports 2 HD", "TNT Sports HD",
    "MBC masrHD", "MBC masr2HD", "ssc1 hd", "ssc2 hd", "Shahid MBC",
]
# Ù†Ø¶ÙŠÙ Ø¨Ø°ÙƒØ§Ø¡ Ù†Ø³Ø® Ø¨Ø¯ÙˆÙ† "HD" Ø£Ùˆ ØªØ¨Ø§ÙŠÙ†Ø§Øª Ø·ÙÙŠÙØ©
_supported_tokens = set()
for c in SUPPORTED_CHANNELS:
    _supported_tokens.add(c.lower())
    _supported_tokens.add(c.lower().replace(" hd", ""))
SUPPORTED_TOKENS = list(_supported_tokens)

def is_supported_channel(name: str) -> bool:
    if not name:
        return False
    n = name.lower()
    return any(tok in n for tok in SUPPORTED_TOKENS)

# === Ù‚ÙˆØ§Ù…ÙŠØ³ Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ÙØ±Ù‚ (Ù„ØªØ®ÙÙŠÙ Ø§Ù„ØªØ±Ø¬Ù…Ø©) ===
TEAM_MAP_EN2AR = {
    # England
    "Manchester City": "Ù…Ø§Ù†Ø´Ø³ØªØ± Ø³ÙŠØªÙŠ", "Arsenal": "Ø£Ø±Ø³Ù†Ø§Ù„",
    "Manchester United": "Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯", "Liverpool": "Ù„ÙŠÙØ±Ø¨ÙˆÙ„",
    "Chelsea": "ØªØ´ÙŠÙ„Ø³ÙŠ", "Tottenham Hotspur": "ØªÙˆØªÙ†Ù‡Ø§Ù…", "Tottenham": "ØªÙˆØªÙ†Ù‡Ø§Ù…",
    "Newcastle United": "Ù†ÙŠÙˆÙƒØ§Ø³Ù„ ÙŠÙˆÙ†Ø§ÙŠØªØ¯", "Aston Villa": "Ø£Ø³ØªÙˆÙ† ÙÙŠÙ„Ø§",
    # Spain
    "Real Madrid": "Ø±ÙŠØ§Ù„ Ù…Ø¯Ø±ÙŠØ¯", "Barcelona": "Ø¨Ø±Ø´Ù„ÙˆÙ†Ø©",
    "Atletico Madrid": "Ø£ØªÙ„ØªÙŠÙƒÙˆ Ù…Ø¯Ø±ÙŠØ¯", "Athletic Bilbao": "Ø£ØªÙ„ØªÙŠÙƒ Ø¨ÙŠÙ„Ø¨Ø§Ùˆ",
    "Real Sociedad": "Ø±ÙŠØ§Ù„ Ø³ÙˆØ³ÙŠØ¯Ø§Ø¯", "Sevilla": "Ø¥Ø´Ø¨ÙŠÙ„ÙŠØ©", "Valencia": "ÙØ§Ù„Ù†Ø³ÙŠØ§",
    # Italy
    "Inter": "Ø¥Ù†ØªØ± Ù…ÙŠÙ„Ø§Ù†", "Inter Milan": "Ø¥Ù†ØªØ± Ù…ÙŠÙ„Ø§Ù†",
    "AC Milan": "Ù…ÙŠÙ„Ø§Ù†", "Milan": "Ù…ÙŠÙ„Ø§Ù†", "Juventus": "ÙŠÙˆÙÙ†ØªÙˆØ³",
    "Napoli": "Ù†Ø§Ø¨ÙˆÙ„ÙŠ", "Roma": "Ø±ÙˆÙ…Ø§", "Lazio": "Ù„Ø§ØªØ³ÙŠÙˆ", "Fiorentina": "ÙÙŠÙˆØ±Ù†ØªÙŠÙ†Ø§",
    "Atalanta": "Ø£ØªØ§Ù„Ø§Ù†ØªØ§", "Bologna": "Ø¨ÙˆÙ„ÙˆÙ†ÙŠØ§", "Torino": "ØªÙˆØ±ÙŠÙ†Ùˆ",
    # Germany / France
    "Bayern Munich": "Ø¨Ø§ÙŠØ±Ù† Ù…ÙŠÙˆÙ†Ø®", "Borussia Dortmund": "Ø¨ÙˆØ±ÙˆØ³ÙŠØ§ Ø¯ÙˆØ±ØªÙ…ÙˆÙ†Ø¯",
    "Paris Saint-Germain": "Ø¨Ø§Ø±ÙŠØ³ Ø³Ø§Ù† Ø¬ÙŠØ±Ù…Ø§Ù†", "PSG": "Ø¨Ø§Ø±ÙŠØ³ Ø³Ø§Ù† Ø¬ÙŠØ±Ù…Ø§Ù†",
    # Portugal
    "Benfica": "Ø¨Ù†ÙÙŠÙƒØ§", "Porto": "Ø¨ÙˆØ±ØªÙˆ", "Sporting CP": "Ø³Ø¨ÙˆØ±ØªÙŠÙ†Øº Ù„Ø´Ø¨ÙˆÙ†Ø©", "Sporting": "Ø³Ø¨ÙˆØ±ØªÙŠÙ†Øº Ù„Ø´Ø¨ÙˆÙ†Ø©",
    # Example from your sample:
    "Hellas Verona": "Ù‡ÙŠÙ„Ø§Ø³ ÙÙŠØ±ÙˆÙ†Ø§", "Cremonese": "ÙƒØ±ÙŠÙ…ÙˆÙ†ÙŠØ²ÙŠ",
}
TEAM_MAP_AR2EN = {v: k for k, v in TEAM_MAP_EN2AR.items()}

def translate_en_to_ar(name: str) -> str:
    if not name:
        return ""
    if name in TEAM_MAP_EN2AR:
        return TEAM_MAP_EN2AR[name]
    if GoogleTranslator:
        try:
            t = GoogleTranslator(source="en", target="ar").translate(name)
            return (t or name).strip()
        except Exception:
            return name
    return name

def translate_ar_to_en(name: str) -> str:
    if not name:
        return ""
    if name in TEAM_MAP_AR2EN:
        return TEAM_MAP_AR2EN[name]
    if GoogleTranslator:
        try:
            t = GoogleTranslator(source="ar", target="en").translate(name)
            return (t or name).strip()
        except Exception:
            return name
    return name

def parse_title_to_teams_generic(title: str) -> tuple[str | None, str | None]:
    """
    ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Home/Away Ù…Ù† title Ø¨Ø¹Ø¯Ø© ÙÙˆØ§ØµÙ„ Ø´Ø§Ø¦Ø¹Ø©.
    """
    if not title:
        return None, None
    t = title.strip()
    # Ø§Ù„ÙÙˆØ§ØµÙ„ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© (Ù…Ø±ØªØ¨Ø© Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ù‹Ø§)
    DELIMS = [
        r"\s+v(?:s)?\.?\s+",   # "v" Ø£Ùˆ "vs" Ø£Ùˆ "vs."
        r"\s+-\s+",            # " - "
        r"\s+â€“\s+",            # en dash
        r"\s+â€”\s+",            # em dash
        r"\s*:\s*",            # " : "
        r"\s*\|\s*",           # " | "
        r"\s*Â·\s*",            # " Â· "
        r"\s*;\s*",            # " ; "
    ]
    for d in DELIMS:
        parts = re.split(d, t, maxsplit=1)
        if len(parts) == 2:
            left, right = parts[0].strip(), parts[1].strip()
            if left and right:
                return left, right
    return None, None

def extract_liveonsat_match_teams(m: dict) -> tuple[str | None, str | None]:
    # Ø£ÙˆÙ„Ù‹Ø§ Ø¬Ø±Ù‘Ø¨ Ø­Ù‚ÙˆÙ„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù† ÙˆØ¬Ø¯Øª
    home = (m.get("home") or m.get("home_team"))
    away = (m.get("away") or m.get("away_team"))
    if home and away:
        return str(home).strip(), str(away).strip()
    # fallback: Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    title = (m.get("title") or "").strip()
    h, a = parse_title_to_teams_generic(title)
    return h, a

# Ù‚Ù†ÙˆØ§Øª ÙŠÙ„Ø§ Ø´ÙˆØª
SPLIT_RE = re.compile(r"\s*(?:,|ØŒ|/|\||&| Ùˆ | and )\s*", re.I)

def to_list_channels(val):
    if isinstance(val, list):
        return [str(x).strip() for x in val if str(x).strip()]
    if isinstance(val, str):
        if not val.strip():
            return []
        return [p.strip() for p in SPLIT_RE.split(val) if p.strip()]
    return []

def collect_yalla_channels(yalla_match: dict) -> list:
    keys_try = ["channels_raw","channels","tv_channels","channel","channel_ar","channel_en","broadcasters","broadcaster"]
    out = []
    for k in keys_try:
        if k in yalla_match:
            out.extend(to_list_channels(yalla_match.get(k)))
    return unique_preserving(out)

def pick_primary_yalla_channel(chs: list[str]) -> str | None:
    if not chs:
        return None
    for c in chs:
        if is_bein_channel(c):
            return c.strip()
    return chs[0].strip()

# Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ liveonsat: Ù‚Ø§Ø¦Ù…Ø© Ø³Ø¬Ù„Ø§Øª (EN home, EN away, [channels])
def build_liveonsat_entries(live_data: dict) -> list[tuple[str,str,list[str]]]:
    entries = []
    matches = (live_data or {}).get("matches", []) or []
    for m in matches:
        h_en, a_en = extract_liveonsat_match_teams(m)
        if not h_en or not a_en:
            continue

        # Ø§Ø¬Ù…Ø¹ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ù…Ù† Ø¹Ø¯Ø© Ø­Ù‚ÙˆÙ„ Ù…Ø­ØªÙ…Ù„Ø©
        raw_channels = []
        for ck in ("channels_raw","channels","tv_channels","broadcasters","broadcaster"):
            if ck in m and m[ck]:
                raw = m[ck]
                if isinstance(raw, list):
                    raw_channels.extend([str(x) for x in raw])
                elif isinstance(raw, str):
                    raw_channels.extend(to_list_channels(raw))

        # Ù†Ø¸Ù‘Ù ÙˆÙÙ„ØªØ± Ø§Ù„Ù‚Ù†ÙˆØ§Øª: Ø´ÙŠÙ„ beINØŒ Ø®Ù„ÙŠÙ‡ ÙÙ‚Ø· Ù…Ù† Ù‚Ø§Ø¦Ù…ØªÙƒ
        filtered = []
        for ch in raw_channels:
            if not ch: 
                continue
            ch_clean = clean_channel_display(ch)
            if not ch_clean:
                continue
            if is_bein_channel(ch_clean):
                continue
            if is_supported_channel(ch_clean):
                filtered.append(ch_clean)

        filtered = unique_preserving(filtered)
        if filtered:
            entries.append((str(h_en).strip(), str(a_en).strip(), filtered))
    return entries

def ok_ratio(a: str, b: str) -> int:
    if not a or not b:
        return 0
    if fuzz:
        return fuzz.token_set_ratio(a, b)
    return 100 if a == b else 0

def match_live_to_yalla(home_ar: str, away_ar: str, lons_entries: list[tuple[str,str,list[str]]]) -> list[str]:
    """
    Ù†Ø­Ø§ÙˆÙ„ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø§Ø±Ø§Ø© ÙŠÙ„Ø§ (AR) Ù…Ø¹ Ø³Ø·ÙˆØ± liveonsat (EN):
      - EN->AR Ù…Ù‚Ø§Ø±Ù†Ø© Ø¹Ø±Ø¨ÙŠ-Ø¹Ø±Ø¨ÙŠ
      - AR->EN Ù…Ù‚Ø§Ø±Ù†Ø© Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ-Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ
      - Ù†Ù‚Ø¨Ù„ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ (homeâ†”away)
    """
    hy_ar = normalize_text(home_ar); ay_ar = normalize_text(away_ar)
    home_en_from_ar = translate_ar_to_en(home_ar); away_en_from_ar = translate_ar_to_en(away_ar)
    hy_en = normalize_text(home_en_from_ar); ay_en = normalize_text(away_en_from_ar)

    extra = []
    for (h_en, a_en, chans) in lons_entries:
        # EN->AR
        h_ar_guess = translate_en_to_ar(h_en); a_ar_guess = translate_en_to_ar(a_en)
        h_ar_norm = normalize_text(h_ar_guess); a_ar_norm = normalize_text(a_ar_guess)
        r1 = min(ok_ratio(h_ar_norm, hy_ar), ok_ratio(a_ar_norm, ay_ar))
        r2 = min(ok_ratio(h_ar_norm, ay_ar), ok_ratio(a_ar_norm, hy_ar))
        match_ar = max(r1, r2)

        # AR->EN
        h_en_norm = normalize_text(h_en); a_en_norm = normalize_text(a_en)
        r3 = min(ok_ratio(h_en_norm, hy_en), ok_ratio(a_en_norm, ay_en))
        r4 = min(ok_ratio(h_en_norm, ay_en), ok_ratio(a_en_norm, hy_en))
        match_en = max(r3, r4)

        best = max(match_ar, match_en)
        if best >= FUZZY_THRESHOLD:
            extra.extend(chans)

    return unique_preserving(extra)

# === Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===
def filter_matches():
    # 1) ÙŠÙ„Ø§ Ø´ÙˆØª
    try:
        yresp = requests.get(YALLASHOOT_URL, timeout=25)
        yresp.raise_for_status()
        yalla = yresp.json()
    except Exception as e:
        print(f"[x] ERROR fetching yallashoot: {e}")
        return

    yalla_matches = (yalla or {}).get("matches", []) or []
    if not yalla_matches:
        print("[!] yallashoot empty.")
        with OUTPUT_PATH.open("w", encoding="utf-8") as f:
            json.dump({"date": yalla.get("date"), "source_url": YALLASHOOT_URL, "matches": []}, f, ensure_ascii=False, indent=2)
        return

    # 2) liveonsat Ù…Ø­Ù„ÙŠ
    try:
        with INPUT_PATH.open("r", encoding="utf-8") as f:
            live_data = json.load(f)
    except Exception as e:
        print(f"[!] WARNING reading liveonsat: {e}")
        live_data = {}

    lons_entries = build_liveonsat_entries(live_data)

    # 3) Ø¯Ù…Ø¬
    out_matches = []
    used_extra = 0
    for m in yalla_matches:
        home_ar = (m.get("home") or m.get("home_team") or "").strip()
        away_ar = (m.get("away") or m.get("away_team") or "").strip()
        if not home_ar or not away_ar:
            continue

        # Ù‚Ù†Ø§Ø© Ù…Ù† ÙŠÙ„Ø§ Ø´ÙˆØª
        y_chs = collect_yalla_channels(m)
        primary = pick_primary_yalla_channel(y_chs)
        yalla_only = [primary] if primary else []

        # Ù‚Ù†ÙˆØ§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† liveonsat
        extra = match_live_to_yalla(home_ar, away_ar, lons_entries)
        if extra:
            used_extra += 1

        channels = unique_preserving([*yalla_only, *extra])

        new_entry = {
            "competition": m.get("competition") or m.get("league") or m.get("tournament"),
            "kickoff_baghdad": m.get("kickoff_baghdad") or m.get("time_baghdad") or m.get("kickoff"),
            "home_team": home_ar,
            "away_team": away_ar,
            "channels_raw": channels,
            "home_logo": m.get("home_logo"),
            "away_logo": m.get("away_logo"),
            "status_text": m.get("status_text"),
            "result_text": m.get("result_text"),
        }
        out_matches.append(new_entry)

    output = {"date": yalla.get("date"), "source_url": YALLASHOOT_URL, "matches": out_matches}
    with OUTPUT_PATH.open("w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"[âœ“] Done. Matches: {len(out_matches)} | Added-extra-from-liveonsat: {used_extra} | threshold={FUZZY_THRESHOLD}")
    if GoogleTranslator is None:
        print("[!] deep-translator not installed â€” install it for best matching.")
    if fuzz is None:
        print("[!] rapidfuzz not installed â€” install it to enable fuzzy matching.")

if __name__ == "__main__":
    filter_matches()
